
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Employee Question     â”‚
â”‚  "What is the leave policy?"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Query Processing       â”‚
â”‚ - Clean question           â”‚
â”‚ - Pass as-is to pipeline   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Document Retrieval (RAG Retriever)     â”‚
â”‚                                          â”‚
â”‚  1ï¸âƒ£ Semantic Search (FAISS)              â”‚
â”‚     â†’ fetch top-k chunks                 â”‚
â”‚                                          â”‚
â”‚  2ï¸âƒ£ Intent-aware Filtering               â”‚
â”‚     â†’ keep only leave-related chunks     â”‚
â”‚                                          â”‚
â”‚  3ï¸âƒ£ Context Limiting                     â”‚
â”‚     â†’ top 3â€“4 chunks only                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Context Injection     â”‚
â”‚                            â”‚
â”‚  Context string injected   â”‚
â”‚  into ChatPromptTemplate   â”‚
â”‚                            â”‚
â”‚  {context} + {question}    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Answer Generation    â”‚
â”‚  (ChatGroq â€“ LLaMA)        â”‚
â”‚                            â”‚
â”‚  â€¢ Reads ONLY context      â”‚
â”‚  â€¢ No guessing             â”‚
â”‚  â€¢ No hallucination        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Answer + Sources    â”‚
â”‚                            â”‚
â”‚  âœ” Grounded                â”‚
â”‚  âœ” Accurate                â”‚
â”‚  âœ” Explainable             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


User Question
     â†“
answer_question(question)
     â†“
LCEL RAG Chain
     â†“
{
  "context": retriever(question),
  "question": question
}
     â†“
retriever()
  â”œâ”€ semantic_search()
  â”œâ”€ intent_filter()
  â”œâ”€ build_context()
     â†“
ChatPromptTemplate
     â†“
ChatGroq LLM
     â†“
StrOutputParser
     â†“
Final Answer

â€œRAG is retrieval first, reasoning later â€” the LLM never sees the database, only the curated context.â€

ğŸ§  YOUR RAG â€“ FULL FLOW (Box-by-Box)
ğŸŸ¦ 1ï¸âƒ£ Knowledge Base (Static World)
data/
 â”œâ”€â”€ Company_Guidelines.txt
 â”œâ”€â”€ HR_Policies.txt


What it is

Raw business knowledge

PDFs / TXT / DOCs

Rule

LLM never sees these files directly âŒ

ğŸŸ¦ 2ï¸âƒ£ Ingestion (ONE-TIME job)
docs_load.py

Files
  â†“
Text
  â†“
Chunks


What happens

Read files

Split into chunks (size + overlap)

Add metadata (source, page)

ğŸ§  Think: â€œPreparing food before cookingâ€

ğŸŸ¦ 3ï¸âƒ£ Embeddings (Meaning â†’ Numbers)
"Employees accrue PTO..." â†’ [0.023, 0.91, -0.44, ...]


What happens

Each chunk â†’ vector

Captures semantic meaning, not keywords

ğŸ§  Think: â€œGPS coordinates for meaningâ€

ğŸŸ¦ 4ï¸âƒ£ Vector Store (FAISS) ğŸ’¾
FAISS Index
 â”œâ”€â”€ vector â†’ chunk
 â”œâ”€â”€ vector â†’ chunk


What happens

Store vectors + text

Saved to disk

Loaded at runtime

âœ… Fast
âœ… Scalable
âœ… No re-ingestion needed

ğŸ§  RUNTIME FLOW (When User Asks a Question)
ğŸŸ¨ 5ï¸âƒ£ User Question
"What is the company's leave policy?"

ğŸŸ¨ 6ï¸âƒ£ Semantic Search (Retriever Layer)

ğŸ“‚ retrieval/retriever.py

question
   â†“
embedding
   â†“
FAISS.similarity_search(k=8)
   â†“
top-K documents


Key rules

Retriever:

âŒ No LLM

âŒ No prompts

âŒ No answers

âœ… Only fetch relevant chunks

ğŸ§  Think: â€œSearch engine, not brainâ€

ğŸŸ¨ 7ï¸âƒ£ Intent-Aware Filtering (RAG Quality Booster)

ğŸ“‚ rag_pipeline.py

Retrieved docs
   â†“
filter_documents_by_intent()
   â†“
ONLY leave-related chunks


Why this matters

Prevents unrelated sections:

ethics

discipline

compensation

ğŸ§  This is why your final answer became clean.

ğŸŸ¨ 8ï¸âƒ£ Context Builder (LLM-safe)
Document objects
   â†“
build_context()
   â†“
Single clean text string

6.1 PTO ...
6.2 Sick Leave ...
6.3 Other Leave ...


Why

LLM understands text, not objects

Control size + ordering

ğŸŸ¦ 9ï¸âƒ£ LCEL RAG Chain (The Brain Wiring)

ğŸ“‚ rag_pipeline.py

{
  "context": retriever,
  "question": RunnablePassthrough()
}
   â†“
Prompt
   â†“
LLM
   â†“
Output Parser


ğŸ§  This is the heart of modern LangChain

ğŸŸ¦ ğŸ”Ÿ Prompt (Rules for the Brain)

ğŸ“‚ prompts.py

You are an HR assistant.
Answer ONLY from context.
If not found, say "I don't know".


Purpose

Prevent hallucination

Enforce grounding

Control tone & format

ğŸŸ¦ 1ï¸âƒ£1ï¸âƒ£ LLM (ChatGroq)
Context + Question
        â†“
Reasoned Answer


Important

LLM does NOT search

LLM does NOT know FAISS

LLM only reasons over provided context

ğŸŸ¦ 1ï¸âƒ£2ï¸âƒ£ Final Answer ğŸ¯
Clean
Relevant
Grounded
No noise


Exactly what you saw ğŸ‘‡

6.1 PTO
6.2 Sick Leave
6.3 Other Leave Types

ğŸ§  ONE-SCREEN MEMORY DIAGRAM (SAVE THIS)
FILES
  â†“
CHUNKS
  â†“
EMBEDDINGS
  â†“
FAISS (Stored)

USER QUESTION
  â†“
Semantic Search (Retriever)
  â†“
Intent Filtering
  â†“
Context Builder
  â†“
PROMPT + CONTEXT + QUESTION
  â†“
LLM
  â†“
FINAL ANSWER

ğŸ¯ Interview One-Line Explanation (VERY IMPORTANT)

â€œWe ingest documents once, store them in a FAISS vector DB, retrieve relevant chunks using semantic search, refine them with intent-aware filtering, build a clean context, and pass that into an LCEL-based RAG chain where the LLM generates grounded answers.â€

ğŸ”¥ That is senior-level wording.

ğŸ§­ What you are ready for now

You can confidently move to:

âœ… Multi-query RAG

âœ… Metadata filters

âœ… Tool calling

âœ… Agentic RAG




